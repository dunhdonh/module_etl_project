# transform_data/transform_fao_long.py
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit
from pyspark.sql.types import FloatType
import os
import shutil
import re

# === 1. Config ===
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
DATA_PATH = os.path.join(BASE_DIR, "input_data", "data_fao")
OUTPUT_DIR = os.path.join(BASE_DIR, "output_data", "fao_long_cleaned")

if os.path.exists(OUTPUT_DIR):
    shutil.rmtree(OUTPUT_DIR)
os.makedirs(OUTPUT_DIR, exist_ok=True)

# === 2. Kh·ªüi t·∫°o Spark ===
spark = SparkSession.builder.appName("FAO Wide to Long").getOrCreate()
spark.sparkContext.setLogLevel("WARN")

csv_files = [os.path.join(DATA_PATH, f) for f in os.listdir(DATA_PATH) if f.endswith(".csv")]
if not csv_files:
    raise FileNotFoundError("‚ùå Kh√¥ng c√≥ file CSV n√†o trong data_input/data_fao")

df_all = None

# === 3. X·ª≠ l√Ω t·ª´ng file CSV ===
for file_path in csv_files:
    print(f"[INFO] üîç ƒêang x·ª≠ l√Ω file: {os.path.basename(file_path)}")
    df = spark.read.option("header", True).csv(file_path)

    # T√¨m c√°c c·ªôt d·∫°ng "YYYY value" v√† "YYYY flag"
    value_cols = [c for c in df.columns if re.match(r"^\d{4} value$", c)]
    flag_cols = [c for c in df.columns if re.match(r"^\d{4} flag$", c)]

    if not value_cols:
        print(f"[WARN] ‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y c·ªôt nƒÉm trong {file_path}. C√°c c·ªôt hi·ªán c√≥:")
        print(df.columns)
        continue

    # === 4. Chuy·ªÉn t·ª´ng nƒÉm th√†nh d·∫°ng long ===
    df_long_parts = []
    for val_col in value_cols:
        year = int(val_col.split()[0])
        flag_col = f"{year} flag"

        df_y = df.select(
            col("Country Name En").alias("country"),
            col("Unit Name").alias("unit"),
            lit(year).alias("year"),
            col(val_col).cast(FloatType()).alias("value"),
            col(flag_col).alias("flag")
        )
        df_long_parts.append(df_y)

    # H·ª£p t·∫•t c·∫£ c√°c nƒÉm trong 1 file
    df_long = df_long_parts[0]
    for d in df_long_parts[1:]:
        df_long = df_long.unionByName(d)

    # Lo·∫°i b·ªè null
    df_long_clean = df_long.dropna()

    # Th√™m v√†o df_all t·ªïng
    df_all = df_long_clean if df_all is None else df_all.unionByName(df_long_clean)

# === 5. Xu·∫•t k·∫øt qu·∫£ theo t·ª´ng nƒÉm ===
if df_all is None:
    raise ValueError("‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu h·ª£p l·ªá sau khi transform.")

years = [r["year"] for r in df_all.select("year").distinct().collect()]
print(f"[INFO] T·ªïng s·ªë nƒÉm: {len(years)} ({min(years)} ‚Üí {max(years)})")

for year in years:
    year_str = str(year)
    temp_dir = os.path.join(OUTPUT_DIR, f"temp_{year_str}")
    final_csv = os.path.join(OUTPUT_DIR, f"{year_str}.csv")

    df_all.filter(col("year") == year) \
        .repartition(1) \
        .write.option("header", True).mode("overwrite").csv(temp_dir)

    # ƒê·ªïi t√™n file CSV
    for f in os.listdir(temp_dir):
        if f.endswith(".csv"):
            os.replace(os.path.join(temp_dir, f), final_csv)
    shutil.rmtree(temp_dir)

spark.stop()
print("‚úÖ Transform ho√†n t·∫•t, d·ªØ li·ªáu ƒë√£ l∆∞u trong output_data/fao_long_cleaned/")
